import numpy as np #tested with version 1.26.4
import uproot #tested with version 5.3.1
import matplotlib.pyplot as plt 
import yaml
import sys 
import awkward as ak #tested with version 2.6.1
import bitstruct
from datetime import datetime
import scipy.interpolate as spi 
from statsmodels.nonparametric.smoothers_lowess import lowess
import os
import pickle
import Util
import error_codes as ec

global chan_rearrange 
chan_rearrange = np.array([5,4,3,2,1,0,11,10,9,8,7,6,17,16,15,14,13,12,23,22,21,20,19,18,29,28,27,26,25,24])

class Acdc:
	def __init__(self, config_data):
		# Loads configuration file. First checks if it's a yaml file; otherwise treats as Python dict
		if isinstance(config_data, str):
			try:
				with open(config_data, 'r') as yf:
					config_data = yaml.safe_load(yf)
			except FileNotFoundError:
				print(f'{config_data} doesn\'t exist')
				config_data = None
		elif not isinstance(config_data, dict):
			print(f'`config_data` file-type not recognized: {type(config_data)}')
			config_data = None

		self.c = config_data
		
		

		#for every event, we have a set of string indexed metadata and waveform data
		#that is packaged as an awkward array. If the dictionary index is "waves", its a
		#numpy array of shape (30, 256) (channel number, sample number)
		#self.events[evno]["sys_time"] = 1234567890
		#self.events[evno][waves][ch] = np.array([signal values])
		self.events = None #will be populated either by loading from file or importing raw data
		###########List of columns for quick reference##############:
		#sys_time: clock metadata variable from each event. 320 MHz counter, 
		# synchronous to 40 MHz sampling clock PSEC. It resets when powered on or system is reset. 
		#There is also an explicit command that can be reset at will from data logging software. 
		#
		#wr_time: white rabbit clock metadata from each event. 2x 32-bit numbers, one of which 
		#is 250 MHz counter that is generated by a 100 MHz output of the WR ZEN module, PLLed by FPGA
		# to 250 MHz, and other word is a 1 Hz counter. 250 MHz counter is reset every 1 Hz count.  
		#NOTE TO SELF: lowest sig bits are fast, higher bits are 1 Hz. 
		#
		#evt_count: the index of the event within the file that it is contained
		#filename: the name of the file which this event came from
		#file_timestamp: the timestamp of the file which this event came from, if it is in the filename
		#channel numbers: the channel number of the waveform data, integer key
		########################################################


		#we have a mapping that corrects the time base of waveforms. 
		#indexed by channel
		self.times = [] 
		#a mapping of shape (30, 256, 4096) that converts the 4096 possible ADC values
		# each capacitor can have to mV. Saving it here so that if it exists, its only loaded
		#once. 
		self.adc_to_mv = [] 
		

		#data reduction structures. Initialized when
		#data reduction is started. 
		self.rq_confit = None
		self.rqs = {} #the output of data reduction. 
		self.rolled = False #a flag to indicate if the waveforms have been rolled.




	#############Pre reduction functions####################

	#This is a temporary method that should be more generalized,
	#as it now requires us to have a specific filename format for all runs. 
	def get_timestamp_from_filename(self, filename):
		filename = filename.split("/")[-1] #in case it has full path
		uns = filename.split("_")
		dt = datetime.strptime(uns[2]+" "+uns[3], "%Y%m%d %H%M%S")
		return dt.timestamp()


	#takes a list of filepaths to individual raw datafiles
	def load_raw_data_to_events(self, infiles):
		# CompiledFormat objects, used as quick shortcuts when unpacking data (instead of having to type "u16p48" each time)
		format_time=bitstruct.compile("u64")
		format_header=bitstruct.compile("u16p48")
		format_accheader=bitstruct.compile("u56u8")
		format=bitstruct.compile("u12"*(256*30))
		NUM64BITWORDS = 1440 # 1440 64-bit words in a single event
		swapformat="8"*(NUM64BITWORDS)

		#re-arrange channel numbers ASAP to be in order of strip number,
		#as opposed to how they are now labled by chip, going left to right
		#and then cycling over 
		infiles = sorted(infiles)
		temp_events = []
		#loop over each raw data file
		for fidx, raw_data_path in enumerate(infiles):
			print("Loading {:d} of {:d} files".format(fidx+1, len(infiles)))
			# Open the raw data file
			event_index = 0
			with open(raw_data_path, "rb") as f:
				line = f.read((1+4+NUM64BITWORDS)*8)
				# This loop breaks when the line length is no longer
				# what we expect it to be (i.e. (1+4+NUM64BITWORDS)*8), 
				# which indicates end of file or file is corrupted (missing portions)
				while(len(line) == (1+4+NUM64BITWORDS)*8):
					acc_header = format_accheader.unpack(bitstruct.byteswap("8", line[0*8:1*8]))
					header = format_header.unpack(bitstruct.byteswap("8", line[1*8:2*8]))
					if((acc_header[0] != 0x123456789abcde) or (header[0] != 0xac9c)):
						#print("CORRUPT EVENT!!! ", lnum, "%x"%acc_header[0], "%x"%header[0])
						line = f.read((1+4+NUM64BITWORDS)*8)
						continue

					sys_time = format_time.unpack(bitstruct.byteswap("8", line[2*8:3*8]))[0]
					wr_time = format_time.unpack(bitstruct.byteswap("8", line[3*8:4*8]))[0]
					data = format.unpack(bitstruct.byteswap(swapformat, line[5*8:]))
					
					#reformat the times and data and pack into our data structure
					temp = {}
					temp["sys_time"] = sys_time #number of 320 MHz clock cycles
					temp["wr_time"] = wr_time #a combined 2 x 32 bit numbers that are the WR time and PPS
					#a hint to the above may be that the old code had
					#((wr_time>>32) & 0xffffffff) + 1e-9*(4*(wr_time & 0xffffffff))
					temp["filename"] = raw_data_path.split("/")[-1]
					temp["evt_count"] = event_index
					temp["file_timestamp"] = self.get_timestamp_from_filename(temp["filename"])


					data = np.array(data).reshape([30, 256]) #take from 1 list into 30 lists
					#rearrange to match channel number to strip number
					data = data[chan_rearrange,:]
					temp["waves"] = data #(30, 256)
					temp_events.append(temp)

					line = f.read((1+4+NUM64BITWORDS)*8)
					event_index += 1
		
		#package into an awkward array
		self.events = ak.Array(temp_events)

	#in contrast to the above function, this takes
	#a single file, with the same structure as a typical file of events,
	#and only returns the shape (n, 30, 256) array of waveform data
	#with no additional metadata. It is typically used for ped data processing. 
	def load_pedestal_data(self, infile):
		#copying what we do above, but not populating the same data structures
		format_header=bitstruct.compile("u16p48")
		format_accheader=bitstruct.compile("u56u8")
		format=bitstruct.compile("u12"*(256*30))
		NUM64BITWORDS = 1440 # 1440 64-bit words in a single event
		swapformat="8"*(NUM64BITWORDS)

		
		#loop over each raw data file
		ped_events = []
		# Open the raw data file
		with open(infile, "rb") as f:
			line = f.read((1+4+NUM64BITWORDS)*8)
			# This loop breaks when the line length is no longer
			# what we expect it to be (i.e. (1+4+NUM64BITWORDS)*8), 
			# which indicates end of file or file is corrupted (missing portions)
			while(len(line) == (1+4+NUM64BITWORDS)*8):
				acc_header = format_accheader.unpack(bitstruct.byteswap("8", line[0*8:1*8]))
				header = format_header.unpack(bitstruct.byteswap("8", line[1*8:2*8]))
				if((acc_header[0] != 0x123456789abcde) or (header[0] != 0xac9c)):
					#print("CORRUPT EVENT!!! ", lnum, "%x"%acc_header[0], "%x"%header[0])
					line = f.read((1+4+NUM64BITWORDS)*8)
					continue

				ped_events.extend(format.unpack(bitstruct.byteswap(swapformat, line[5*8:])))
				line = f.read((1+4+NUM64BITWORDS)*8)

		#reshape the data array for quick averaging
		ped_events = np.array(ped_events).reshape([-1, 30, 256])
		ped_events = ped_events[:, chan_rearrange, :]
		return ped_events



	def calibrate_waveforms(self, pedfiles):
		#first check if any data is loaded
		if(self.events is None):
			print("No data loaded. Cannot calibrate waveforms.")
			return
		#a dictionary indexed by the filename of the data/events file
		#for which the pedestal data corresponds to. The values of the dict
		#are a (30, 256) array of averaged pedestal values for each channel. 
		processed_pedestals = {} 

		#but even if there are many files for the events data, there may be 
		#only one pedestal file to load. For each unique data file, we will associate
		#the appropriate pedestal file. 
		pedfile_for_datafile = {} #key is datafile, value is pedfilename

		#a numpy array of (30, 256, 4096) that converts the 4096 possible ADC values
		# each capacitor can have to mV
		adc_to_mv = [] 


		##############timebase#################################
		#check if the timebase data has already been loaded.
		if(len(self.times) == 0):
			#load the timebase data
			print("Loading timebase data")
			self.load_timebase_data()


		###############pedestal part first######################
		#get a set of what file names and timestamps are in the events data. 
		#we don't want to be loading ALL pedestal files for nothing, and we only
		#want to load them once. 
		unique_datafiles = list(set(self.events["filename"]))
		unique_timestamps = [self.get_timestamp_from_filename(f) for f in unique_datafiles]
		pedfile_timestamps = [self.get_timestamp_from_filename(f) for f in pedfiles]
		#for each input datafile, find the pedfile that is closest in time without going over
		for i in range(len(unique_datafiles)):
			t0 = unique_timestamps[i]
			selected_pedfile = pedfiles[np.argmin([t0-t for t in pedfile_timestamps if t0-t >= 0])]
			pedfile_for_datafile[unique_datafiles[i]] = selected_pedfile
		
		#load the pedestal files that are needed. 
		for datafilename in pedfile_for_datafile:
			if(datafilename in processed_pedestals): continue
			print("Loading pedestal data for file {}".format(pedfile_for_datafile[datafilename]))
			#load that pedestal data into the processed_pedestals dictionary, where the
			#mean over axis=0 will average over all events for each capacitor
			processed_pedestals[datafilename] = self.load_pedestal_data(pedfile_for_datafile[datafilename]).mean(axis=0)
			

		##############adc linearity part########################
		#check if the lineary data has already been loaded. 
		if(len(self.adc_to_mv) != 0):
			adc_to_mv = self.adc_to_mv
		else:
			print("Loading linearity data")
			adc_to_mv = self.load_linearity_data()
		
		#the pedestal data is in ADC counts, and can be linearity corrected 
		#to mV by using the adc_to_mv array.
		print("Calibrating pedestal data to convert to mV")
		for k in processed_pedestals.keys():
			new_pedestals = np.zeros_like(processed_pedestals[k])
			for ch in range(len(adc_to_mv)):
				for cap in range(len(adc_to_mv[ch])):
					#need to interpolate the transfer function because the pedestals are
					#the result of an average of many events, and thus are not integers. 
					s = spi.interp1d(np.arange(4096), adc_to_mv[ch][cap], kind="linear")
					new_pedestals[ch][cap] = s(processed_pedestals[k][ch][cap])
			processed_pedestals[k] = new_pedestals #Now in mV

		#now we can calibrate the waveforms by first converting to mV and then
		#pedestal subtracting. 
		print("Pedestal subtracting and converting to mV for all event waveforms")
		#This part takes the absolute longest, however I don't know how to vectorize it.
		#If anyone can vectorize this and re-frame the data back into the original structure,
		#please take on that challenge. Presently it is about 1.5 events per second on my local machine. 
		all_new_waves = []
		for i, ev in enumerate(self.events):
			print("On event {:d} of {:d}".format(i, len(self.events)))
			#find the pedestal data for this event
			ped = processed_pedestals[ev["filename"]]
			waves = ev["waves"]
			#the calibrated waves (to modify)
			new_waves = np.zeros((30, 256))

			#uses "fancy indexing" to (1) select values along the last dimension of adc_to_mv,
			#which is the mV associated with each ADC value,. The waves[..., np.newaxis] part 
			#reshapes waves to be (30, 256, 1) to match the shape of adc_to_mv indexing. 
			#This results in reshaped_adc_to_mv being adc_to_mv[ch, cap, waves[ch, cap]] for each ch and cap. 
			reshaped_adc_to_mv = np.take_along_axis(adc_to_mv, waves[...,np.newaxis], axis=2)[...,0]
			new_waves = reshaped_adc_to_mv - ped
		
			all_new_waves.append(new_waves)

		#replace the old waveforms with the new ones
		self.events = ak.with_field(self.events, all_new_waves, where="waves")



	#saves a numpy array that maps the 4096 possible ADC values to mV
	#for each capacitor and each channel. into self.adc_to_mv[ch][cap][adc] = mV
	def load_linearity_data(self):
		#check for the calibration root file's existence, which
		#comes from the configuration file for the board. 
		if("calib_file_name" in self.c):
			calib_file = self.c["calib_file_name"]
			#check if it exists
			if(os.path.exists(calib_file)):
				#load the file. NOTE: if you have an issue
				#finding the branch adc_to_mv in the config_tree, you likely
				#have not "preprocessed" the linearity scans, which can be found
				#in "scripts/ReprocessLinearityCurves.ipynb"
				with uproot.open(calib_file) as f:
					self.adc_to_mv = np.reshape(f["config_tree"]["adc_to_mv"].array(library="np"), (30,256,4096))
					#rearrange the channel numbers to match the order of the strips
					self.adc_to_mv = self.adc_to_mv[chan_rearrange, :, :]

				return self.adc_to_mv #in case the user wants to use it right away or check for Nones
			else:
				print("Can't find the filepath for the calibration file: {}	".format(calib_file))
				adc_to_mv = np.zeros((30, 256, 4096))
				for ch in range(len(adc_to_mv)):
					for cap in range(len(adc_to_mv[ch])):
						adc_to_mv[ch][cap] = np.arange(4096)*1200.0/4096.0
				self.adc_to_mv = adc_to_mv
				return self.adc_to_mv
		else:	
			print("No calibration file key in configuration file. Please create a key called 'calib_file_name' with the path to the calibration file.")
			adc_to_mv = np.zeros((30, 256, 4096))
			for ch in range(len(adc_to_mv)):
				for cap in range(len(adc_to_mv[ch])):
					adc_to_mv[ch][cap] = np.arange(4096)*1200.0/4096.0
			self.adc_to_mv = adc_to_mv
			return self.adc_to_mv

	#loads the timebase data from the root file
	def load_timebase_data(self):
		if("calib_file_name" in self.c):
			calib_file = self.c["calib_file_name"]
			#check if it exists
			if(os.path.exists(calib_file)):
				#load the file. NOTE: if you have an issue
				#finding the branch adc_to_mv in the config_tree, you likely
				#have not "preprocessed" the linearity scans, which can be found
				#in "scripts/ReprocessLinearityCurves.ipynb"
				with uproot.open(calib_file) as f:
					tb = np.reshape(f["config_tree"]["time_offsets"].array(library="np"), (30,256))

				#rearrange the channels
				tb = tb[chan_rearrange, :]
				#put in units of nanoseconds
				self.times = tb*1e9
				return self.times #in case they use right away
			else:
				print("Can't find the filepath for the calibration file: {}	".format(calib_file))
				dt = 1.0/(40e6*256)*1e9
				self.times = np.array([[i*dt for i in range(256)] for j in range(30)])
				return self.times
		else:
			print("No calibration file key in configuration file. Please create a key called 'calib_file_name' with the path to the calibration file.")
			dt = 1.0/(40e6*256)*1e9
			self.times = np.array([[i*dt for i in range(256)] for j in range(30)])
			return self.times
		
	#a quick function to populate default times, sometimes needed when timebase data is wierd
	def load_default_times(self):
		dt = 1.0/(40e6*256)*1e9
		self.times = np.array([[i*dt for i in range(256)] for j in range(30)])

	#I attemped to have file writing and reading use root trees. I failed. 
	#For now I am pickling the stuff. 
	def write_events_to_file(self, outpath):
		#check that outpath has a .root extension
		if(outpath.split(".")[-1] != "p"):
			print("Output file must have a .p extension. Not saving!")
			#I choose to force the user to change their driving code instead
			#of fixing the filename, so that they are well aware of this requirement. 
			return

		pickle.dump([{"events":self.events, "times":self.times}], open(outpath, "wb"))


	def read_events_from_file(self, inpath):
		#check that inpath has a .root extension
		if(inpath.split(".")[-1] != "p"):
			print("Input file must have a .p extension. Not loading!")
			#I choose to force the user to change their driving code instead
			#of fixing the filename, so that they are well aware of this requirement. 
			return

		d = pickle.load(open(inpath, "rb"))[0]
		self.events = d["events"]
		self.times = d["times"]


	####################End prereduction functions#################################



	####################reduction functions###################################

	def initialize_rqs(self):
		#the output data structure contains many reduced
		#quantities that form an event indexed dataframe. 
		#load the yaml file containing these reduced quantities
		#notes on reduced data output structure
		try:
			with open(self.c["rq_file"], 'r') as yf:
				self.rq_config = yaml.safe_load(yf)
		except FileNotFoundError:
			print(f'{self.c["rq_file"]} doesn\'t exist')
			self.rq_config = None
		
		self.rqs = {}
		#initialize the global event branches (not channel specific)
		for key, str_type in self.rq_config["global"].items():
			self.rqs[key] = []

		#initialize the channel specific branches
		for ch in range(30):
			for key, str_type in self.rq_config["channel"].items():
				self.rqs["ch{:d}_{}".format(ch, key)] = []

	def roll_waveforms(self):
		#Splits the ring buffer into octants and finds a lower bound for the octant that the trigger is in
		if self.rolled:
			print("Waveforms already rolled. Not rolling again.")
			return
		self.rolled = True
		BUFFER_LENGTH = 256
		OCTANT_LENGTH = BUFFER_LENGTH//8 #number of samples in the octant
		NUM_OCTANTS = 8
		CLOCK_OFFSET = self.c["clock_offset"] #offset in clock cycles to account for the time it takes to process the trigger.

		#JOE: Please comment on each element of this calculation
		trigger_low_bound = (((self.events["sys_time"]+CLOCK_OFFSET)%NUM_OCTANTS)*OCTANT_LENGTH - (0))%BUFFER_LENGTH
		self.rqs["start_cap"] = trigger_low_bound
		# Roll all waveforms
		wave = np.array(self.events["waves"])
		
		rolled_waves = [np.roll(wave[ev], -trigger_low_bound[ev], axis=1) for ev in range(len(trigger_low_bound))]

		# Replace the old waveforms with the rolled waveforms
		self.events["waves"] = np.array(rolled_waves)

	def reduce_data(self):

		############initialization functions##############
		#initialize the reduced quantities data structure
		self.initialize_rqs()

		#populate that RQ structure with some carryover quantities
		#from the self.events array that were just from raw files. 
		self.rqs["evt_count"] = self.events["evt_count"]
		self.rqs["filename"] = self.events["filename"]
		self.rqs["file_timestamp"] = self.events["file_timestamp"]
		self.rqs["sys_time"] = self.events["sys_time"]
		#In the future, it would be wise to decompose the two 32 bit words
		#of wr_time during pre-reduction. But for now, I include it here. 
		wr_times = np.array(self.events["wr_time"])
		self.rqs["pps"] = ((wr_times >> 32) & 0xffffffff) #the 1 Hz counter referencing 250 MHz WR ZEN
		self.rqs["wr_time"] = (wr_times & 0xffffffff) #the 250 MHz counter from WR ZEN that resets every 1 second
		self.rqs["error_codes"] = [[] for i in range(len(self.events["sys_time"]))] #a list of error codes for each event. sys_time is just an example column to get the length of the list.
		#####event vectorized operations############

		#uses information on the trigger time to roll
		#the buffer of the waveforms to be causal
		self.roll_waveforms()

		#analyze baselines, populate baseline values and STDs,
		#and baseline subtract waveforms. Uses a coarse, vectorized
		#analysis that knows nothing about peak locations. Takes
		#input on baseline samples from the configuration files. 
		self.analyze_and_subtract_baselines()

		#find the maximum amplitude channel for each event
		self.find_minmax_of_channels()

		#get the full standard deviation of the whole waveform
		self.find_full_std()

		#find channels that seem to be hit by pulses that are reconstructable
		self.find_hit_chs()

		self.reconstruct_peak_time(verbose=True)

		#For each event, mark the channel which is optimal for reconstructing the arrival time, by looking at their peak amplitudes.
		self.identify_peak_channels(verbose=True)

		#find the phase info on WR channels
		self.construct_wr_phi(verbose=True)

		#########event looped operations###############
		for i, ev in enumerate(self.events):
			#find the peak locations and amplitudes
			#self.reconstruct_peaks(ev)
			pass
	###########################Analysis functions####################################
	def calc_longitudinal_pos(self):
		pass


	def find_minmax_of_channels(self):
		waves = np.array(self.events["waves"])

		max_values = np.percentile(waves, 95, axis=2)#For robustness, we use the 95th percentile
		min_values = np.percentile(waves, 5, axis=2)#For robustness, we use the 5th percentile
		for ch in range(30):
			self.events["ch{}_max".format(ch)] = max_values[:, ch]
			self.events["ch{}_min".format(ch)] = min_values[:, ch]

	def find_full_std(self):
		waves = np.array(self.events["waves"])
		full_std_values = np.std(waves, axis=2)
		for ch in range(30):
			self.events["ch{}_full_std".format(ch)] = full_std_values[:, ch]


	def analyze_and_subtract_baselines(self):
		waves = np.array(self.events["waves"])

		#calculate the number of samples for the baseilne before the pulse. 
		dt = 1.0/(40e6*256)*1e9 #time difference between samples in ns
		baseline_samples = int(self.c["baseline_ns"]/dt)
		baselines = np.apply_along_axis(Util.find_baseline_simple, 2, waves, baseline_samples)
		baseline_std_values = np.apply_along_axis(Util.find_baseline_std_simple, 2, waves, baseline_samples)

		for ch in range(30):
			self.events["ch{}_baseline".format(ch)] = baselines[:, ch]
			self.events["ch{}_baseline_std".format(ch)] = baseline_std_values[:, ch]

		#subtract the baseline from the waveforms
		self.events["waves"] = waves - baselines[:, :, np.newaxis]


	#Populate everything except the peak times, which are event looped and much slower
	def find_hit_chs(self):
		for ch in range(30):
			#A simple logic to determine if a channel is hit.
			is_hits = np.abs(self.events["ch{}_min".format(ch)] / self.events["ch{}_baseline_std".format(ch)]) > self.c["hit_threshold"] #hit_threshold should typically be a number between 3 and 5.
			self.events["ch{}_is_hit".format(ch)] = is_hits


	def reconstruct_peak_time(self, verbose = False):	
		"""
		Reconstructs the peak time for each channel in each event.
		"""
		waves = np.array(self.events["waves"])
		for ch in range(30):
			min_values = self.events["ch{}_min".format(ch)]
			start_caps = self.rqs["start_cap"]
			peak_times_ch = []
			success = 0
			#Apply peak finding to events whose is_hit is true and populate the peak info.
			#This can be parallelized in the future as well.
			for ev, is_hit in enumerate(self.events["ch{}_is_hit".format(ch)]):
				if (is_hit):
					try:
						#The sign of waves is flipped because the peak finding function finds the peak of the negative of the waveform.
						peak_times_ch.append(Util.find_peak_time_inflection(ydata = -waves[ev, ch], y_robust_min = min_values[ev], x_start_cap = start_caps[ev], timebase_ns= self.times[ch]))
						success += 1
					except ValueError as e:
						if(verbose):
							print("Peak finding failed for event {:d} on channel {:d}, due to ValueError: {:s}".format(ev, ch, str(e)))
						peak_times_ch.append([-1, -1])
						# TODO: This error is channel specific, so we should not populate the global error_codes which affects the entire event.
						# self.rqs["error_codes"][ev].append(ec.Station_Error.PEAK_FIND_FAIL)
					except IndexError as e:
						if(verbose):
							print("Peak finding failed for event {:d} on channel {:d}, due to IndexError: {:s}".format(ev, ch, str(e)))
						peak_times_ch.append([-1, -1])
						# TODO: This error is channel specific, so we should not populate the global error_codes which affects the entire event.
						# self.rqs["error_codes"][ev].append(ec.Station_Error.PEAK_FIND_FAIL)
					except TypeError as e:
						if(verbose):
							print("Peak finding failed for event {:d} on channel {:d}, due to TypeError: {:s}".format(ev, ch, str(e)))
						peak_times_ch.append([-1, -1])
						# TODO: This error is channel specific, so we should not populate the global error_codes which affects the entire event.
						# self.rqs["error_codes"][ev].append(ec.Station_Error.PEAK_FIND_FAIL)
				else:
					peak_times_ch.append([-1, -1])#We have to append something to keep the length of the array consistent with the number of events.
			if verbose:
				print("Populated peak times for channel {:d}".format(ch))
				#Print the number of non default peak times to check for errors
				print("Number of events with non-default peak times for channel {:d}: {:d}".format(ch, success))
			self.events["ch{}_peak_times".format(ch)] = peak_times_ch

	def identify_peak_channels(self, verbose = False):
		"""
		Identifies the channel with the maximum amplitude for each event, after reconstruction of the peak time.
		"""
		peak_ch = np.argmin(np.array([self.events["ch{}_min".format(ch)] for ch in range(30)]), axis = 0)
		self.rqs["peak_ch"] = peak_ch
		if verbose:
			print("Populated peak_ch.")

	def construct_wr_phi(self, verbose = False):
		"""
		Constructs the phase of the WR signal for each event.
		Must be run after identify_peak_channels.
		"""
		#We assume that peak_ch has been populated. peak_ch must have exactly two peaks.
		#First, we find the average peak time for each event, and also create a mask for events with exactly two peaks.
		avg_peak_time = []
		#Create masks for events with exactly two peaks, as we will only analyze these events.
		two_peaks_mask = []
		self.rqs["time_measured_ch"] = np.zeros(len(self.events["ch0_is_hit"]))
		for ev in range(len(self.events["ch0_is_hit"])):#Peculiarly, the length of the is_hit arrays is not the number of events
			if self.events["ch{}_is_hit".format(int(self.rqs["peak_ch"][ev]))][ev] == 1:
				ch = self.rqs["peak_ch"][ev]
			else:
				#If peak_ch is not populated or the corresponding channel is not hit (this is a weird situation), find the first channel with is_hit = 1
				for ch in range(30):
					if self.events["ch{}_is_hit".format(ch)][ev] == 1:
						break
				self.rqs["error_codes"][ev].append(ec.Station_Error.IMPROPER_PEAK_CH)#Arbitrary error code for improper peak_ch. Not used anywhere else in the code.
			self.rqs["time_measured_ch"][ev] = int(ch)
			avg_peak_time.append(np.mean(self.events["ch{}_peak_times".format(ch)][ev]))
			two_peaks_mask.append(len(self.events["ch{}_peak_times".format(ch)][ev]) == 2)

		##################################################################################################################################################

		#Using Util.find_sine_phase to find the phase of the WR signal
		#and populate the rqs with the results.
		waves = np.array(self.events["waves"])
		success = 0
		self.rqs["wr_phi"] = np.zeros_like(waves[:, 0, 0])

		#Fit details for debugging
		self.rqs["wr_Phi0"] = np.zeros_like(waves[:, 0, 0])
		self.rqs["wr_Amplitude"] = np.zeros_like(waves[:, 0, 0])
		self.rqs["wr_Offset"] = np.zeros_like(waves[:, 0, 0])

		for ev, is_particle in enumerate(two_peaks_mask):
			if (is_particle):
				try:
					ch = int(self.rqs["time_measured_ch"][ev])
					popt = Util.find_sine_phase(ydata = waves[ev, self.c["sync_ch"]], timebase_ns = self.times[ch], ydata_max = self.events["ch{}_max".format(self.c["sync_ch"])][ev], x_start_cap = self.rqs["start_cap"][ev], samples_after_zero = self.c["sine_fit_exclusion_samples_after_zero"],  samples_before_end = self.c["sine_fit_exclusion_samples_before_end"])
					self.rqs["wr_phi"][ev] = popt[0] + avg_peak_time[ev]/4*2*np.pi #The phase of the WR signal is the phase of the sine wave at the time of the point between the two peaks.

					#Fit details for debugging
					self.rqs["wr_Phi0"][ev] = popt[0]
					self.rqs["wr_Amplitude"][ev] = popt[1]
					self.rqs["wr_Offset"][ev] = popt[2]

					success += 1
				except ValueError:
					self.rqs["error_codes"][ev].append(ec.Station_Error.SIN_FIT_VALUE_ERROR)#Fit value error
				except RuntimeError:
					self.rqs["error_codes"][ev].append(ec.Station_Error.SIN_FIT_FAIL)#Fit fail
		if verbose:
			print("Populated WR phi.")
			#Print the number of non default peak times to check for errors
			print("Number of events with non-default WR phi: {:d}".format(success))

		

			
				
			
	




			



