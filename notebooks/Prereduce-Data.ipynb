{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append(\"../Acdc/\")\n",
    "sys.path.append(\"../Util/\")\n",
    "sys.path.append(\"../Analysis/\")\n",
    "sys.path.append(\"../WaveformPlotter/\")\n",
    "sys.path.append(\"../Errorcodes/\")\n",
    "import Acdc \n",
    "import Analysis\n",
    "import WaveformPlotter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionaries for loading configurations nicely\n",
    "### Also define some filepaths that should be adjusted based on your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of this dict:\n",
    "#key: acdc number\n",
    "#value: dict with the following keys\n",
    "#   obj: Acdc object itself\n",
    "#   config: configuration file path \n",
    "#   infiles: list of input data files for that board\n",
    "\n",
    "#this set of boards is used in the July 2024 proton data at test-beam\n",
    "acdcs = {44:{\"obj\":None, \"config\":\"../configs/acdc44.yml\", \"infiles\": [], \"pedfiles\": []}, \\\n",
    "\t43:{\"obj\":None, \"config\":\"../configs/acdc43.yml\", \"infiles\": [], \"pedfiles\": []}}\n",
    "\n",
    "#Configure some data filepaths\n",
    "#you'll likely want to keep paths as we may be committing/pushing\n",
    "#multiple paths. Just use a comment\n",
    "datadir = \"../../data/20240701ProtonDataFTBF/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the waveforms into the events attribute\n",
    "for acdc_num, a in acdcs.items():\n",
    "\ta[\"obj\"] = Acdc.Acdc(a[\"config\"])\n",
    "\n",
    "\t#the configuration is now parsed, so we can find which station id\n",
    "\t#in order to parse the filetag at the end of the data filenames. \n",
    "\tbnum = a[\"obj\"].c[\"station_id\"]\n",
    "\t#find all the data files for this board\n",
    "\ta[\"infiles\"] = glob.glob(datadir + f\"Raw_Proton*b{bnum}.txt\")\n",
    "\t#find all the pedestal files for this board\n",
    "\ta[\"pedfiles\"] = glob.glob(datadir + f\"Raw_test*b{bnum}.txt\")\n",
    "\n",
    "\n",
    "\t#go file by file and save pre-reduced data for each file.\n",
    "\t#The machinery in the ACDC class can instead handle all files,\n",
    "\t# it is so resource and RAM intensive that I instead opt to save\n",
    "\t#a pre-reduced output file for each input file, looping individually. \n",
    "\tfor f in a[\"infiles\"]:\n",
    "\t\t#the function can take a list of files, \n",
    "\t\t#so I pass just the one we are working on\n",
    "\t\ta[\"obj\"].load_raw_data_to_events([f]) \n",
    "\n",
    "\t\t#loads root file containing linearity data, \n",
    "\t\t#calibrates pedestals based on ADC/voltage and does\n",
    "\t\t#pedestal subtraction, and loads timebase calibration.\n",
    "\t\t#Within these pedfiles, it finds the closest file to our\n",
    "\t\t#events without using a future pedfile. \n",
    "\t\ta[\"obj\"].calibrate_waveforms(a[\"pedfiles\"])\n",
    "\t\ta[\"obj\"].write_events_to_file(f.replace(\".txt\", \"_prereduced.p\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pre-reduced data\n",
    "#load the waveforms into the events attribute\n",
    "for acdc_num, a in acdcs.items():\n",
    "    a[\"obj\"] = Acdc.Acdc(a[\"config\"])\n",
    "\n",
    "    #the configuration is now parsed, so we can find which station id\n",
    "    #in order to parse the filetag at the end of the data filenames. \n",
    "    bnum = a[\"obj\"].c[\"station_id\"]\n",
    "    if bnum == 1:\n",
    "        a[\"obj\"].read_events_from_file(\"../../LAPPDTestData/Raw_ProtonData_20240701_134140_b1_prereduced.p\")\n",
    "    elif bnum == 0:\n",
    "        a[\"obj\"].read_events_from_file(\"../../LAPPDTestData/Raw_ProtonData_20240701_134140_b0_prereduced.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256):\n",
    "    acdcs[43][\"obj\"].times[0][i] = acdcs[43][\"obj\"].times[1][i] #In my acdc43.root file, ch 0 is corrupted, so I copy ch 1 to ch 0\n",
    "for acdc_num, a in acdcs.items():\n",
    "    a[\"obj\"].reduce_data(event_mask = np.arange(0,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Util\n",
    "\n",
    "wp = WaveformPlotter.WaveformPlotter([acdcs[44][\"obj\"], acdcs[43][\"obj\"]])\n",
    "\n",
    "tgt = acdcs[44][\"obj\"]\n",
    "#print(acdcs[44][\"obj\"].times[11])\n",
    "plt.style.use(\"~/evanstyle.mplstyle\") #change this if you want! or comment\n",
    "plt.close()\n",
    "for ev in range(200, 230):\n",
    "    if(tgt.rqs[\"time_measured_ch\"][ev]!= 0):\n",
    "        fig, ax = wp.plot_waveforms_separated(station_id=0, event_number=ev)\n",
    "        plt.vlines(tgt.rqs[\"ch{}_peak_times\".format(int(tgt.rqs[\"time_measured_ch\"][ev]))][ev], 0, 1000, color=\"red\")\n",
    "        print(int(tgt.rqs[\"time_measured_ch\"][ev]))\n",
    "        print(int(tgt.rqs[\"peak_ch\"][ev]))\n",
    "        print(tgt.rqs[\"ch{}_peak_times\".format(int(tgt.rqs[\"time_measured_ch\"][ev]))][ev])\n",
    "        print(tgt.rqs[\"wr_phi\"][ev])\n",
    "        print(tgt.rqs[\"error_codes\"][ev])\n",
    "        plt.plot(np.linspace(0, 25, 256), np.sin(np.linspace(0, 25, 256)/2*np.pi+tgt.rqs[\"wr_Phi0\"][ev])*tgt.rqs[\"wr_Amplitude\"][ev] + tgt.rqs[\"wr_Offset\"][ev])\n",
    "        #plt.savefig(\"../../plots/example_waveforms/{:d}_b0.png\".format(i))\n",
    "        plt.show()\n",
    "\n",
    "#plt.plot(np.linspace(0, 25, 256), tgt.events[\"waves\"][1100][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def gaussianfunc(p,x):\n",
    "    return p[0]/(p[2]*np.sqrt(2*np.pi))*np.exp(-(x-p[1])**2/(2*p[2]**2))+p[3]\n",
    "\n",
    "def gaussianfuncSuperposed(p,x):\n",
    "    return p[0]/(p[2]*np.sqrt(2*np.pi))*np.exp(-(x-p[1])**2/(2*p[2]**2))+p[4]/(p[2]*np.sqrt(2*np.pi))*np.exp(-(x-p[5])**2/(2*p[2]**2))+p[3]\n",
    "\n",
    "def gaussiandelta(p, x):\n",
    "    return gaussianfunc(p[0:3],x) + delta(p[3],x)\n",
    "\n",
    "def delta(p,x):\n",
    "    return x\n",
    "\n",
    "def residual(p,func, xvar, yvar, err):\n",
    "    return np.array((func(p, xvar) - yvar)/err)\n",
    "\n",
    "def data_fit(p0, func, xvar, yvar, err, tmi=0):\n",
    "    try:\n",
    "        fit = optimize.least_squares(residual, p0, args=(func,xvar, yvar, err), verbose=tmi)\n",
    "    except Exception as error:\n",
    "        print(\"Something has gone wrong:\",error)\n",
    "        return p0, np.zeros_like(p0), np.nan, np.nan\n",
    "    pf = fit['x']\n",
    "\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        cov = np.linalg.inv(fit['jac'].T.dot(fit['jac']))          \n",
    "        # This computes a covariance matrix by finding the inverse of the Jacobian times its transpose\n",
    "        # We need this to find the uncertainty in our fit parameters\n",
    "    except:\n",
    "        # If the fit failed, print the reason\n",
    "        print('Fit did not converge')\n",
    "        print('Result is likely a local minimum')\n",
    "        print('Try changing initial values')\n",
    "        print('Status code:', fit['status'])\n",
    "        print(fit['message'])\n",
    "        return pf, np.zeros_like(pf), np.nan, np.nan\n",
    "            #You'll be able to plot with this, but it will not be a good fit.\n",
    "\n",
    "    chisq = sum(residual(pf, func, xvar, yvar, err) **2)\n",
    "    dof = len(xvar) - len(pf)\n",
    "    red_chisq = chisq/dof\n",
    "    pferr = np.sqrt(np.diagonal(cov)) # finds the uncertainty in fit parameters by squaring diagonal elements of the covariance matrix\n",
    "    print('Converged with chi-squared {:.2f}'.format(chisq))\n",
    "    print('Number of degrees of freedom, dof = {:.2f}'.format(dof))\n",
    "    print('Reduced chi-squared {:.2f}'.format(red_chisq))\n",
    "    print()\n",
    "    Columns = [\"Parameter #\",\"Initial guess values:\", \"Best fit values:\", \"Uncertainties in the best fit values:\"]\n",
    "    print('{:<11}'.format(Columns[0]),'|','{:<24}'.format(Columns[1]),\"|\",'{:<24}'.format(Columns[2]),\"|\",'{:<24}'.format(Columns[3]))\n",
    "    for num in range(len(pf)):\n",
    "        print('{:<11}'.format(num),'|','{:<24.3e}'.format(p0[num]),'|','{:<24.3e}'.format(pf[num]),'|','{:<24.3e}'.format(pferr[num]))\n",
    "    return pf, pferr, chisq,dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform CFD amd Peak time separation analysis\n",
    "\n",
    "tgt = acdcs[44][\"obj\"]\n",
    "#print(acdcs[44][\"obj\"].times[11])\n",
    "plt.style.use(\"~/evanstyle.mplstyle\") #change this if you want! or comment\n",
    "plt.close()\n",
    "peak_time_separation = []\n",
    "for ev in range(230, 250):\n",
    "    if(tgt.rqs[\"time_measured_ch\"][ev]!= 0):\n",
    "        ch = int(tgt.rqs[\"time_measured_ch\"][ev])\n",
    "        if (tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0] > 0):\n",
    "            interpolated_zero = np.interp(tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0],tgt.times_rolled[ev][ch], tgt.events[\"waves\"][ev][ch])\n",
    "            plt.plot(tgt.times_rolled[ev][ch] - tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0], (tgt.events[\"waves\"][ev][ch]) / tgt.rqs[\"ch{}_amplitudes\".format(ch)][ev])\n",
    "            #plt.plot(tgt.times_rolled[ev][ch] - tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0], tgt.events[\"waves\"][ev][ch] )\n",
    "\n",
    "            #Print baseline parameters next to the waveform\n",
    "            #First get the first point of the plot\n",
    "            x_0 = tgt.times_rolled[ev][ch][0] - tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0]\n",
    "            y_0 = (tgt.events[\"waves\"][ev][ch][0]) / tgt.rqs[\"ch{}_amplitudes\".format(ch)][ev]\n",
    "            y_0 = tgt.events[\"waves\"][ev][ch][0]\n",
    "\n",
    "            #plt.text(x_0, y_0, \"trigger_end:{:.2f}\".format(tgt.rqs[\"corrupted_samples_from_begin\"][ev]), fontsize=6,verticalalignment='top')\n",
    "            #plt.text(x_0, y_0, \"{:.2f}, {:.2f}\".format(tgt.rqs[\"ch{}_baseline\".format(ch)][ev], tgt.rqs[\"ch{}_baseline_precise\".format(ch)][ev]), fontsize=6,verticalalignment='top')\n",
    "            #Print error codes next to the waveform\n",
    "            #plt.text(x_0, y_0, str(tgt.rqs[\"ch{}_warnings\".format(ch)][ev]), fontsize=6,verticalalignment='bottom')\n",
    "            peak_time_separation.append(tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][1] - tgt.rqs[\"ch{}_peak_times\".format(ch)][ev][0])\n",
    "        \n",
    "        #plt.savefig(\"../../plots/example_waveforms/{:d}_b0.png\".format(i))\n",
    "plt.xlim(-4, 4)\n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Normalized Amplitude\")\n",
    "plt.show()\n",
    "peak_time_separation = np.array(peak_time_separation)\n",
    "print(np.mean(peak_time_separation))\n",
    "print(np.std(peak_time_separation))\n",
    "\n",
    "plt.hist(peak_time_separation, bins=300)\n",
    "plt.xlabel(\"Time Difference $t_2 - t_1$ (ns)\")\n",
    "plt.ylabel(\"Counts, total {}\".format(np.size(peak_time_separation)))\n",
    "plt.title(\"Peak separation of one station\")\n",
    "\n",
    "y2, bin_edges = np.histogram(peak_time_separation, bins=300)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "plt.xlim(3,6)\n",
    "\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(3, 50, textfit , fontsize=12,verticalalignment='top')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.plot(np.linspace(0, 25, 256), tgt.events[\"waves\"][1100][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time of flight between two stations\n",
    "\n",
    "an = Analysis.Analysis(\"../configs/analysis.yml\", acdcs)\n",
    "an.construct_tracks(acdcs[44][\"obj\"], acdcs[43][\"obj\"])\n",
    "print(an.tracks[\"error_codes\"])\n",
    "error_mask = [len(an.tracks[\"error_codes\"][trk])==0 for trk in range(len(an.tracks[\"error_codes\"]))]\n",
    "plt.hist(an.tracks[\"time_of_flight_ns\"][error_mask], bins=200)\n",
    "plt.xlabel(\"Time Difference $t_2 - t_1$ (ns)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Subsecond WR Counter Comparison Between Two Stations, After Coincidence Cut\")\n",
    "\n",
    "y2, bin_edges = np.histogram(an.tracks[\"time_of_flight_ns\"][error_mask], bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(0, 100, textfit , fontsize=12,verticalalignment='top')\n",
    "plt.xlim(0, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position distribution of one station\n",
    "\n",
    "filter = [acdcs[44][\"obj\"].rqs[\"hpos\"][ev] > 0 and acdcs[44][\"obj\"].rqs[\"vpos\"][ev] > 0 for ev in range(len(acdcs[44][\"obj\"].rqs[\"hpos\"]))]\n",
    "data_h = np.array(acdcs[44][\"obj\"].rqs[\"hpos\"])[filter]\n",
    "print(np.mean(data_h))\n",
    "print(np.std(data_h))\n",
    "\n",
    "plt.hist(data_h, bins=200)\n",
    "plt.xlabel(\"Horizontal Position (mm)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Horizontal Position Distribution of one station\")\n",
    "\n",
    "y2, bin_edges = np.histogram(data_h, bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "#plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(0, 100, textfit , fontsize=12,verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data_v = np.array(acdcs[44][\"obj\"].rqs[\"vpos\"])[filter]\n",
    "\n",
    "print(np.mean(data_v))\n",
    "print(np.std(data_v))\n",
    "plt.hist(data_v, bins=200)\n",
    "plt.xlabel(\"Vertical Position (mm)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Vertical Position Distribution of one station\")\n",
    "\n",
    "y2, bin_edges = np.histogram(data_v, bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "#plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(0, 100, textfit , fontsize=12,verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track position difference between two stations\n",
    "\n",
    "station1_event_ids =  an.tracks[\"event_ids\"][error_mask][:, 0]\n",
    "station2_event_ids =  an.tracks[\"event_ids\"][error_mask][:,1]\n",
    "data_h = np.array(acdcs[44][\"obj\"].rqs[\"hpos\"])[station1_event_ids] - np.array(acdcs[43][\"obj\"].rqs[\"hpos\"])[station2_event_ids]\n",
    "print(np.mean(data_h))\n",
    "print(np.std(data_h))\n",
    "\n",
    "plt.hist(data_h, bins=200)\n",
    "plt.xlabel(\"Horizontal Position (mm)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Horizontal Position Difference of particle events across two stations\")\n",
    "\n",
    "y2, bin_edges = np.histogram(data_h, bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 10, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}mm$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}mm$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(200, 100, textfit , fontsize=12,verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "data_v = np.array(acdcs[44][\"obj\"].rqs[\"vpos\"])[station1_event_ids] - np.array(acdcs[43][\"obj\"].rqs[\"vpos\"])[station2_event_ids]\n",
    "plt.hist(data_v, bins=200)\n",
    "plt.xlabel(\"Vertical Position (mm)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Vertical Position Difference of particle events across two stations\")\n",
    "\n",
    "y2, bin_edges = np.histogram(data_v, bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 10, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}mm$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}mm$\\n'.format(pf1[2],pferr1[2])\n",
    "plt.text(200, 100, textfit , fontsize=12,verticalalignment='top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Track Angle Analysis\n",
    "\n",
    "print(np.mean(an.tracks[\"polar_angle_phi\"][error_mask]))\n",
    "print(np.std(an.tracks[\"polar_angle_phi\"][error_mask]))\n",
    "\n",
    "plt.hist(an.tracks[\"polar_angle_phi\"][error_mask], bins=200)\n",
    "plt.xlabel(\"Polar Angle Phi (rad)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Subsecond WR Counter Comparison Between Two Stations, After Coincidence Cut\")\n",
    "\n",
    "y2, bin_edges = np.histogram(an.tracks[\"polar_angle_phi\"][error_mask], bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "#plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "#plt.text(0, 100, textfit , fontsize=12,verticalalignment='top')\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(an.tracks[\"polar_angle_theta\"][error_mask]))\n",
    "print(np.std(an.tracks[\"polar_angle_theta\"][error_mask]))\n",
    "plt.hist(an.tracks[\"polar_angle_theta\"][error_mask], bins=200)\n",
    "plt.xlabel(\"Polar Angle Theta (rad)\")\n",
    "plt.ylabel(\"# of Coincidence Pairs, total {}\".format(np.count_nonzero(error_mask)))\n",
    "plt.title(\"Subsecond WR Counter Comparison Between Two Stations, After Coincidence Cut\")\n",
    "\n",
    "y2, bin_edges = np.histogram(an.tracks[\"polar_angle_theta\"][error_mask], bins=200)\n",
    "x2 = (bin_edges[:-1] + bin_edges[1:]) / 2  # Calculate bin centers\n",
    "dy2 = np.sqrt(y2+1)  # Assuming Poisson statistics for the errors\n",
    "p0 = [np.max(y2)/2, x2[np.argmax(y2)], 0.2, 0]  # Initial guess for the fit parameters\n",
    "pf1, pferr1, chisq1, dof1 = data_fit(p0, gaussianfunc, x2, y2, dy2)\n",
    "#plt.plot(x2, gaussianfunc(pf1, x2), 'b-', label='fit',zorder=11)\n",
    "textfit = '$f(x) = N/\\sigma\\sqrt{{2\\pi}}e^{-(x-\\mu)^2/2\\sigma^2}$ \\n' \n",
    "textfit += '$N = {:.2f} \\pm {:.2f}$ counts\\n'.format(pf1[0],pferr1[0]) \n",
    "textfit +='$\\mu = {:.3f} \\pm {:.3f}ns$ \\n'.format(pf1[1],pferr1[1])\n",
    "textfit += '$\\sigma = {:.3f} \\pm {:.3f}ns$\\n'.format(pf1[2],pferr1[2])\n",
    "#plt.text(0, 100, textfit , fontsize=12,verticalalignment='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lappd-tof-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
